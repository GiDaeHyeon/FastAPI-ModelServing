import torch.nn as nnfrom transformers import BertModel as Modelclass BertModel(nn.Module):    def __init__(self,                 weight='kykim/bert-kor-base',                 n_classes=2,                 freeze=True):        super().__init__()        self.model = Model.from_pretrained(weight)        if freeze:            for param in self.model.parameters():                param.require_grad = False        self.clf = nn.Sequential(            nn.Linear(768, 768),            nn.Dropout(p=.3),            nn.LeakyReLU(),            nn.Linear(768, 256),            nn.Dropout(p=.3),            nn.LeakyReLU(),            nn.Linear(256, n_classes)        )        self.activation = nn.LogSoftmax(dim=-1)    def forward(self, input_ids, attention_mask, token_type_ids):        x = self.model(input_ids=input_ids,                       attention_mask=attention_mask,                       token_type_ids=token_type_ids)        x = self.clf(x.pooler_output)        y_hat = self.activation(x)        return y_hat